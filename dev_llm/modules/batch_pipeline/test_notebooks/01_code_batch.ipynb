{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('C:\\Projects\\regionintelligenceai\\dev_llm\\modules\\batch_pipeline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Optional, List\n",
    "\n",
    "import hashlib\n",
    "from pydantic import BaseModel\n",
    "from unstructured.partition.html import partition_html\n",
    "from unstructured.cleaners.core import clean, replace_unicode_quotes, clean_non_ascii_chars\n",
    "from unstructured.staging.huggingface import chunk_by_attention_window\n",
    "from unstructured.staging.huggingface import stage_for_transformers\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm import tqdm\n",
    "from batch_pipeline.utils import TRAINING_DATA_DIR\n",
    "from batch_pipeline.utils import get_console_logger\n",
    "from batch_pipeline.utils import model, tokenizer\n",
    "import os\n",
    "from datetime import datetime\n",
    "from qdrant_client import QdrantClient\n",
    "import json\n",
    "\n",
    "\"\"\"\n",
    "QDRANT_API_URL = os.environ['QDRANT_API_URL']\n",
    "QDRANT_API_KEY = os.environ['QDRANT_API_KEY']\n",
    "\n",
    "\n",
    "\n",
    "# Current Date for file naming and logging\n",
    "current_date = datetime.datetime.now().strftime('%Y%m%d')\n",
    "\n",
    "CALIFORNIA_JSON_FILE = TRAINING_DATA_DIR / f'concatenated_california_building_code_data_{current_date}.json'\n",
    "QDRANT_COLLECTION_NAME = 'california_building_codes'\n",
    "QDRANT_VECTOR_SIZE = 384\n",
    "\n",
    "# init logger\n",
    "logger = get_console_logger()\n",
    "\n",
    "def get_qdrant_client() -> QdrantClient:\n",
    "    \"\"\"\"\"\"\n",
    "    qdrant_client = QdrantClient(\n",
    "        url=QDRANT_API_URL, \n",
    "        api_key=QDRANT_API_KEY,\n",
    "    )\n",
    "\n",
    "    return qdrant_client\n",
    "\n",
    "def init_collection(\n",
    "    qdrant_client: QdrantClient,\n",
    "    collection_name: str,\n",
    "    vector_size: int,\n",
    "    # schema: str = ''\n",
    ") -> QdrantClient:\n",
    "    \"\"\"\"\"\"\n",
    "    from qdrant_client.http.api_client import UnexpectedResponse\n",
    "    from qdrant_client.http.models import Distance, VectorParams\n",
    "\n",
    "    try: \n",
    "        qdrant_client.get_collection(collection_name=collection_name)\n",
    "\n",
    "    except (UnexpectedResponse, ValueError):\n",
    "        qdrant_client.recreate_collection(\n",
    "            collection_name=collection_name,\n",
    "            vectors_config=VectorParams(\n",
    "                size=vector_size,\n",
    "                distance=Distance.COSINE\n",
    "            ),\n",
    "            # schema=schema\n",
    "    )\n",
    "\n",
    "    return qdrant_client\n",
    "\n",
    "\n",
    "class Document(BaseModel):\n",
    "    id: str\n",
    "    group_key: Optional[str] = None\n",
    "    metadata: Optional[dict] = {}\n",
    "    text: Optional[list] = []\n",
    "    chunks: Optional[list] = []\n",
    "    embeddings: Optional[list] = []\n",
    "\n",
    "def parse_document(chapter_data: Dict) -> Document:\n",
    "    try:\n",
    "        document_id = hashlib.md5(str(chapter_data).encode()).hexdigest()\n",
    "        document = Document(id=document_id)\n",
    "        texts = []\n",
    "\n",
    "        for section in chapter_data['sections']:\n",
    "            texts.append(section['title'])\n",
    "            texts.append(section['content'])\n",
    "            for subsection in section['subsections']:\n",
    "                texts.append(subsection['title'])\n",
    "                texts.append(subsection['content'])\n",
    "\n",
    "        joined_text = \" \".join(texts)\n",
    "        document.text = [joined_text]\n",
    "        document.metadata['chapter'] = chapter_data['chapter']\n",
    "        \n",
    "        return document\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error parsing document: {e}\")\n",
    "        return None\n",
    "    \n",
    "def chunk(document: Document) -> Document:\n",
    "    try:\n",
    "        chunks = []\n",
    "        for text in document.text:\n",
    "            chunks += chunk_by_attention_window(\n",
    "                text, tokenizer, max_input_size=QDRANT_VECTOR_SIZE)\n",
    "        \n",
    "        document.chunks = chunks\n",
    "        return document\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error chunking document: {e}\")\n",
    "        return None\n",
    "\n",
    "def embedding(document: Document) -> Document:\n",
    "    try:\n",
    "        for chunk in document.text:\n",
    "            inputs = tokenizer(chunk,\n",
    "                               padding=True,\n",
    "                               truncation=True,\n",
    "                               return_tensors=\"pt\",\n",
    "                               max_length=QDRANT_VECTOR_SIZE)\n",
    "\n",
    "            result = model(**inputs)\n",
    "            embeddings = result.last_hidden_state[:, 0, :].cpu().detach().numpy()\n",
    "            lst = embeddings.flatten().tolist()\n",
    "            document.embeddings.append(lst)\n",
    "        return document\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error while embedding document: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def build_payloads(doc: Document) -> List:\n",
    "    try:\n",
    "        payloads = []\n",
    "        for chunk in doc.chunks:\n",
    "            payload = doc.metadata\n",
    "            payload.update({\"text\": chunk})\n",
    "            payloads.append(payload)\n",
    "        return payloads\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error while building payloads: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def push_document_to_qdrant(doc: Document) -> None:\n",
    "    try:\n",
    "        from qdrant_client.models import PointStruct\n",
    "\n",
    "        _payloads = build_payloads(doc)\n",
    "\n",
    "        qdrant_client.upsert(\n",
    "            collection_name=QDRANT_COLLECTION_NAME,\n",
    "            points=[\n",
    "                PointStruct(\n",
    "                    id=idx,\n",
    "                    vector=vector,\n",
    "                    payload=_payload\n",
    "                )\n",
    "                for idx, (vector, _payload) in enumerate(zip(doc.embeddings, _payloads))\n",
    "            ]\n",
    "        )\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error while pushing document to Qdrant: {e}\")\n",
    "\n",
    "\n",
    "def process_one_building_code(_data: Dict) -> None:\n",
    "    try:\n",
    "        doc = parse_document(_data)\n",
    "        if doc is None:\n",
    "            return None\n",
    "        doc = chunk(doc)\n",
    "        if doc is None:\n",
    "            return None\n",
    "        doc = embedding(doc)\n",
    "        if doc is None:\n",
    "            return None\n",
    "        push_document_to_qdrant(doc)\n",
    "\n",
    "        return doc\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error while processing building code: {e}\")\n",
    "        return None\n",
    "\n",
    "def process_one_document(_data: Dict) -> None:\n",
    "    \"\"\"\"\"\"\n",
    "    try:\n",
    "        doc = parse_document(_data)\n",
    "        if doc:\n",
    "            doc = chunk(doc)\n",
    "            doc = embedding(doc)\n",
    "            push_document_to_qdrant(doc)\n",
    "        return doc\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error while processing one building code document: {e}\")\n",
    "        return None\n",
    "    \n",
    "def embed_building_codes_into_qdrant(building_codes_data: List[Dict], n_processes: int = 1) -> None:\n",
    "    \"\"\"\"\"\"\n",
    "    results = []\n",
    "    try:\n",
    "        if n_processes == 1:\n",
    "            # sequential\n",
    "            for _data in tqdm(building_codes_data):\n",
    "                result = process_one_document(_data)\n",
    "                results.append(result)\n",
    "        else:\n",
    "            # parallel\n",
    "            import multiprocessing\n",
    "\n",
    "            # Create a multiprocessing Pool\n",
    "            with multiprocessing.Pool(processes=n_processes) as pool:\n",
    "                # Use tqdm to create a progress bar\n",
    "                results = list(tqdm(pool.imap(process_one_document, building_codes_data),\n",
    "                                    total=len(building_codes_data),\n",
    "                                    desc=\"Processing\",\n",
    "                                    unit=\"building_code\"))\n",
    "\n",
    "        breakpoint()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error while embedding building codes into Qdrant: {e}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \"\"\"\"\"\"\n",
    "    import json\n",
    "    with open(CALIFORNIA_JSON_FILE, 'r') as json_file:\n",
    "        building_codes_data = json.load(json_file)\n",
    "\n",
    "    embed_building_codes_into_qdrant(\n",
    "        building_codes_data,\n",
    "        n_processes=1\n",
    "    )\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
