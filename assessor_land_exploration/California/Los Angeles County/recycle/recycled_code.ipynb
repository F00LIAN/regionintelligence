{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_instances(df):\n",
    "    \"\"\"\n",
    "    Find instances of mixed data types in a dataframe.\n",
    "    \"\"\"\n",
    "    for column in df.columns:\n",
    "        # Calculate the data types only once per column to improve efficiency\n",
    "        # Treat None/NaN as 'NoneType' explicitly\n",
    "        column_types = df[column].apply(lambda x: type(x).__name__ if x is not None else 'NoneType')\n",
    "        type_counts = column_types.value_counts()\n",
    "\n",
    "        # Print information only for columns with mixed types\n",
    "        if len(type_counts) > 1:\n",
    "            print(f\"Data types in column '{column}':\")\n",
    "            # Print the amount of nulls in the column\n",
    "            null_count = df[column].isnull().sum()\n",
    "       \n",
    "            print(f\"Number of null values in column '{column}': {null_count}\")\n",
    "            print(type_counts)\n",
    "            \n",
    "            # Print an example of an instance for each type and count nulls associated with each type\n",
    "            for t in type_counts.index:\n",
    "                # Filter the dataframe to only rows of the current type\n",
    "                filtered_df = df[column][column_types == t]\n",
    "                # Find an example of the current type\n",
    "                example = filtered_df.iloc[0] if t != 'NoneType' else 'None'\n",
    "                print(f\"Example of type {t}: {example}\")\n",
    "                \n",
    "                # Count nulls specifically for the current type\n",
    "                if t != 'NoneType':\n",
    "                    null_count = filtered_df.isnull().sum()\n",
    "                else:\n",
    "                    null_count = filtered_df.size  # All entries are 'None' for 'NoneType'\n",
    "                \n",
    "                print(f\"Number of null values for type {t} in column '{column}': {null_count}\")\n",
    "\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_city_names(*dicts):\n",
    "    merged_dict = {}\n",
    "    for d in dicts:\n",
    "        for city, names in d.items():\n",
    "            if city not in merged_dict:\n",
    "                merged_dict[city] = set(names)  # Use a set to avoid duplicates\n",
    "            else:\n",
    "                merged_dict[city].update(names)\n",
    "    # Convert sets back to lists\n",
    "    for city in merged_dict:\n",
    "        merged_dict[city] = list(merged_dict[city])\n",
    "    return merged_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_multiple_dfs_on_apn(DS, *dfs):\n",
    "    \"\"\"\n",
    "    Renames 'ain' to 'APN' in the DS DataFrame and merges it with multiple DataFrames on 'APN'.\n",
    "    Saves the rows that did not make the merge into a separate DataFrame.\n",
    "\n",
    "    Args:\n",
    "        DS (pd.DataFrame): The original DataFrame with 'ain' column.\n",
    "        *dfs (pd.DataFrame): Additional DataFrames to merge with DS on 'APN'.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The merged DataFrame.\n",
    "        pd.DataFrame: The DataFrame with dropped rows.\n",
    "    \"\"\"\n",
    "    # Rename 'ain' to 'APN' in DS\n",
    "    DS_renamed = DS.rename(columns={'ain': 'assessor_identification_number'})\n",
    "\n",
    "    # Save the original rows\n",
    "    original_rows = DS_renamed.copy()\n",
    "\n",
    "    # Perform the merge operations sequentially\n",
    "    merged_df = DS_renamed\n",
    "    for df in dfs:\n",
    "        merged_df = merged_df.merge(df, on='assessor_identification_number', how='inner')\n",
    "\n",
    "    # Identify the rows that were dropped\n",
    "    merged_APNs = merged_df['assessor_identification_number']\n",
    "    dropped_rows_df = original_rows[~original_rows['assessor_identification_number'].isin(merged_APNs)]\n",
    "    \n",
    "    return merged_df, dropped_rows_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_datetime(df, datetime_columns):\n",
    "    \"\"\"\n",
    "    Converts specified columns to datetime, handling zero and NaN values.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to process.\n",
    "        datetime_columns (list): List of columns to convert to datetime.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame with the datetime columns processed.\n",
    "    \"\"\"\n",
    "    for column in datetime_columns:\n",
    "        df[column] = pd.to_datetime(df[column].astype(str).replace(['0', '0.0', 'nan', 'NaT', np.nan, 'NaN'], '19700101'), format='%Y%m%d', errors='coerce')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_convert_strings(df, columns):\n",
    "    \"\"\"\n",
    "    Cleans and converts specified columns in a dataframe to strings. It replaces null values, nan, and any instances\n",
    "    of 'ÿ' and its repetitions with 'Unknown', then converts each column to string type.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The dataframe to process.\n",
    "        columns (list of str): The list of column names to process as string columns.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The dataframe with the processed columns.\n",
    "    \"\"\"\n",
    "    for column in columns:\n",
    "        if column in df.columns:\n",
    "            # Replace 'ÿ' sequences and null values with 'Unknown'\n",
    "            df[column] = df[column].replace(to_replace=r'ÿ+', value='Unknown', regex=True).fillna('Unknown')\n",
    "            # Convert to string type\n",
    "            df[column] = df[column].astype(str)\n",
    "        else:\n",
    "            print(f\"Warning: Column '{column}' not found in DataFrame.\")\n",
    "    return df\n",
    "\n",
    "def convert_to_datetime(df, datetime_columns):\n",
    "    \"\"\"\n",
    "    Converts specified columns to datetime, handling zero and NaN values.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to process.\n",
    "        datetime_columns (list): List of columns to convert to datetime.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame with the datetime columns processed.\n",
    "    \"\"\"\n",
    "    for column in datetime_columns:\n",
    "        # Replace invalid entries with a placeholder date\n",
    "        df[column] = pd.to_datetime(\n",
    "            df[column].astype(str).replace(['0', '0.0', 'nan', 'NaT', np.nan, 'NaN'], '19700101'),\n",
    "            format='%Y%m%d', errors='coerce'\n",
    "        ).dt.normalize()  # Normalize to remove the time part\n",
    "    \n",
    "    # Replace NaT values with the placeholder date\n",
    "    df[datetime_columns] = df[datetime_columns].fillna(pd.Timestamp('1970-01-01'))\n",
    "    \n",
    "    return df\n",
    "\n",
    "def convert_to_int(df, columns):\n",
    "    \"\"\"\n",
    "    Converts specified columns in a dataframe to int64. It handles floats by converting them directly to ints,\n",
    "    numeric strings are also converted to ints, and non-numeric strings or any other non-convertible values are\n",
    "    replaced with 0.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The dataframe to process.\n",
    "        columns (list of str): The list of column names to process as integer columns.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The dataframe with the processed columns.\n",
    "    \"\"\"\n",
    "    for column in columns:\n",
    "        if column in df.columns:\n",
    "            # Attempt to convert all values to int64, replacing non-convertible values with 0\n",
    "            df[column] = pd.to_numeric(df[column], errors='coerce').fillna(0).astype('int64')\n",
    "        else:\n",
    "            print(f\"Warning: Column '{column}' not found in DataFrame.\")\n",
    "\n",
    "    return df\n",
    "\n",
    "def convert_to_float(df, columns):\n",
    "    \"\"\"\n",
    "    Converts specified columns in a dataframe to float64. It handles numeric strings and integers by converting them\n",
    "    directly to floats, and non-numeric strings or any other non-convertible values are replaced with 0.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The dataframe to process.\n",
    "        columns (list of str): The list of column names to process as float columns.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The dataframe with the processed columns.\n",
    "    \"\"\"\n",
    "    for column in columns:\n",
    "        if column in df.columns:\n",
    "            # Attempt to convert all values to float64, replacing non-convertible values with 0\n",
    "            df[column] = pd.to_numeric(df[column], errors='coerce').fillna(0).astype('float64')\n",
    "        else:\n",
    "            print(f\"Warning: Column '{column}' not found in DataFrame.\")\n",
    "\n",
    "    return df\n",
    "\n",
    "def detect_column_types_from_dict(df, column_types_dict):\n",
    "    \"\"\"\n",
    "    Detects and verifies the data types for the specified columns using the provided dictionary.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to process.\n",
    "        column_types_dict (dict): A dictionary mapping column names to their desired data types.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary mapping column names to their detected data types.\n",
    "    \"\"\"\n",
    "    column_types = {}\n",
    "    for column, expected_type in column_types_dict.items():\n",
    "        if column in df.columns:\n",
    "            if pd.api.types.is_dtype_equal(df[column], expected_type):\n",
    "                column_types[column] = expected_type\n",
    "            else:\n",
    "                # Mixed type detection (this part could be more sophisticated)\n",
    "                unique_types = set(df[column].apply(type))\n",
    "                if expected_type == 'object' and (str in unique_types or bytes in unique_types):\n",
    "                    column_types[column] = 'object'\n",
    "                elif expected_type == 'int64' and (int in unique_types or float in unique_types):\n",
    "                    column_types[column] = 'int64'\n",
    "                elif expected_type == 'float64' and (float in unique_types or int in unique_types):\n",
    "                    column_types[column] = 'float64'\n",
    "                elif expected_type in ['datetime64'] and pd.api.types.is_datetime64_any_dtype(df[column]):\n",
    "                    column_types[column] = 'datetime64'\n",
    "                else:\n",
    "                    column_types[column] = expected_type\n",
    "        else:\n",
    "            print(f\"Warning: Column '{column}' not found in DataFrame.\")\n",
    "    return column_types\n",
    "\n",
    "def apply_column_conversions(df, column_types):\n",
    "    \"\"\"\n",
    "    Applies the appropriate conversions to the DataFrame columns based on a dictionary of column types.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to process.\n",
    "        column_types (dict): A dictionary mapping column names to their desired data types.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame with the columns converted to the specified data types.\n",
    "    \"\"\"\n",
    "\n",
    "    string_columns = []\n",
    "    int_columns = []\n",
    "    datetime_columns = []\n",
    "    float_columns = []\n",
    "    \n",
    "    # Classify columns by the target data type\n",
    "    for column, dtype in column_types.items():\n",
    "        if dtype == 'object':\n",
    "            string_columns.append(column)\n",
    "        elif dtype == 'int64':\n",
    "            int_columns.append(column)\n",
    "        elif dtype == 'float64':\n",
    "            float_columns.append(column)\n",
    "        elif dtype == 'datetime64':\n",
    "            datetime_columns.append(column)\n",
    "    \n",
    "    # Apply conversions\n",
    "    if string_columns:\n",
    "        df = df.astype({col: 'str' for col in string_columns})\n",
    "    if int_columns:\n",
    "        df[int_columns] = df[int_columns].apply(pd.to_numeric, errors='coerce').fillna(0).astype('int64')\n",
    "    if float_columns:\n",
    "        df[float_columns] = df[float_columns].apply(pd.to_numeric, errors='coerce').astype('float64')\n",
    "    if datetime_columns:\n",
    "        df = convert_to_datetime(df, datetime_columns)\n",
    "\n",
    "    return df\n",
    "\n",
    "def clean_column_names(df):\n",
    "    \"\"\"\n",
    "    Cleans the column names by making them lowercase, replacing spaces with underscores, and removing dashes.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to process.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame with cleaned column names.\n",
    "    \"\"\"\n",
    "    df.columns = df.columns.str.replace('-', '', regex=False)\n",
    "    df.columns = df.columns.str.lower().str.replace(' ', '_').str.replace('-', '_')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
