{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'agenda_link': 'https://riversideca.legistar.com/View.ashx?M=A&ID=1180709&GUID=50BBED8F-EE5D-4287-AB8B-23CC4E37BEE8',\n",
       "  'commission_name': 'Planning Commission',\n",
       "  'meeting_date': None},\n",
       " {'agenda_link': 'https://riversideca.legistar.com/View.ashx?M=A&ID=1180709&GUID=50BBED8F-EE5D-4287-AB8B-23CC4E37BEE8',\n",
       "  'commission_name': 'Housing Authority',\n",
       "  'meeting_date': None}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def filter_commissions(commissions):\n",
    "    filtered_commissions = [item for item in commissions if \"plann\" in item['commission_name'].lower() or \"hous\" in item['commission_name'].lower()]\n",
    "    return filtered_commissions\n",
    "\n",
    "# Example usage with your provided data\n",
    "commissions = [\n",
    "    {'agenda_link': 'https://riversideca.legistar.com/View.ashx?M=A&ID=1162898&GUID=C450F90A-EDA2-4CD6-88BB-DA487619F5ED', 'commission_name': 'Board of Library Trustees', 'meeting_date': None},\n",
    "    {'agenda_link': 'https://riversideca.legistar.com/View.ashx?M=A&ID=1171651&GUID=2579BF25-5DF3-4A8F-A101-0A4CE7D370D8', 'commission_name': 'Board of Ethics', 'meeting_date': None},\n",
    "    {'agenda_link': 'https://riversideca.legistar.com/View.ashx?M=A&ID=1181139&GUID=8334E23A-59F8-42FF-A9B4-03F6FAEE1234', 'commission_name': 'Transportation Board', 'meeting_date': None},\n",
    "    {'agenda_link': 'https://riversideca.legistar.com/View.ashx?M=A&ID=1180709&GUID=50BBED8F-EE5D-4287-AB8B-23CC4E37BEE8', 'commission_name': 'Human Resources Board', 'meeting_date': None},\n",
    "    {'agenda_link': 'https://riversideca.legistar.com/View.ashx?M=A&ID=1180709&GUID=50BBED8F-EE5D-4287-AB8B-23CC4E37BEE8', 'commission_name': 'Planning Commission', 'meeting_date': None},\n",
    "    {'agenda_link': 'https://riversideca.legistar.com/View.ashx?M=A&ID=1180709&GUID=50BBED8F-EE5D-4287-AB8B-23CC4E37BEE8', 'commission_name': 'Housing Authority', 'meeting_date': None}\n",
    "]\n",
    "\n",
    "filtered_commissions = filter_commissions(commissions)\n",
    "filtered_commissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"commissions\": [\n",
      "        {\n",
      "            \"agenda_link\": \"https://riversideca.legistar.com/View.ashx?M=A&ID=1162898&GUID=C450F90A-EDA2-4CD6-88BB-DA487619F5ED\",\n",
      "            \"commission_name\": \"Board of Library Trustees\",\n",
      "            \"meeting_date\": null\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Assuming filter_commissions function is defined as before\n",
    "\n",
    "from pydantic import BaseModel\n",
    "from typing import Optional, List\n",
    "from pydantic import BaseModel, parse_obj_as\n",
    "\n",
    "class Commission(BaseModel):\n",
    "    agenda_link: str\n",
    "    commission_name: str\n",
    "    meeting_date: Optional[str] = None\n",
    "\n",
    "class CityCommissions(BaseModel):\n",
    "    city_name: str\n",
    "    commissions: List[Commission]\n",
    "\n",
    "class CommissionGroups(BaseModel):\n",
    "    commission_groups: List[CityCommissions]\n",
    "\n",
    "class CommissionList(BaseModel):\n",
    "    commissions: List[Commission]\n",
    "\n",
    "commissions_data = [\n",
    "        {'agenda_link': 'https://riversideca.legistar.com/View.ashx?M=A&ID=1162898&GUID=C450F90A-EDA2-4CD6-88BB-DA487619F5ED', 'commission_name': 'Board of Library Trustees', 'meeting_date': None, 'city_name': 'Riverside'},\n",
    "]\n",
    "\n",
    "filtered_commissions = filter_commissions(commissions_data)\n",
    "# Creating Commission instances\n",
    "commission_models = [Commission(**item) for item in commissions_data]\n",
    "commission_list_instance = CommissionList(commissions=commission_models)\n",
    "\n",
    "# Create an instance of CommissionList with the filtered commissions\n",
    "commission_list_instance = CommissionList(commissions=commission_models)\n",
    "\n",
    "# Convert to JSON\n",
    "json_output = commission_list_instance.json(indent=4)\n",
    "print(json_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html_content = \"\"\"\n",
    "<div>\n",
    "    <tr class=\"odd\">\n",
    "        <td class=\"meeting-title bodyTextColour\">Meeting Title 1</td>\n",
    "        <td class=\"bodyTextColour sorting_1\">Meeting Date 1</td>\n",
    "        <td class=\" bodyTextColour\">\n",
    "          <div class=\"docContainer archived-templates\">\n",
    "                <div class=\"left\">\n",
    "                    <a href=\"https://bonkers.com\" class=\"dropdown-document-1\">Document 1</a>\n",
    "                    <a href=\"https://billers.com\" class=\"dropdown-document-2\">Document 2</a>\n",
    "                </div>\n",
    "        </td>\n",
    "        <td class=\"meeting-title bodyTextColour\">Meeting Title</td>\n",
    "        <td class=\"bodyTextColour sorting_1\">Meeting Date</td>\n",
    "        <td class=\" bodyTextColour\">\n",
    "            <div class=\"docContainer archived-templates\">\n",
    "                <div class=\"left\">\n",
    "                    <a href=\"/Public/CompiledDocument?meetingTemplateId=21847&compileOutputType=1\" class=\"dropdown-document-1\"><i> Document 1 </i></a>\n",
    "                    <a href=\"/Public/CompiledDocument?meetingTemplateId=21847&compileOutputType=1\" class=\"dropdown-document-1\"><i> Document 1 </i></a>\n",
    "                </div>\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr class=\"odd\">\n",
    "        <td class=\"meeting-title bodyTextColour\">Meeting Title 2</td>\n",
    "        <td class=\"bodyTextColour sorting_1\">Meeting Date 2</td>\n",
    "        <td class=\" bodyTextColour\">\n",
    "          <div class=\"docContainer archived-templates\">\n",
    "                <div class=\"left\">\n",
    "                    <a href=\"https://juggs.com\" class=\"dropdown-document-1\">Document 1</a>\n",
    "                    <a href=\"https://jizz.com\" class=\"dropdown-document-2\">Document 2</a>\n",
    "                </div>\n",
    "        </td>\n",
    "        <td class=\"meeting-title bodyTextColour\">Meeting Title</td>\n",
    "        <td class=\"bodyTextColour sorting_1\">Meeting Date</td>\n",
    "        <td class=\" bodyTextColour\">\n",
    "            <div class=\"docContainer archived-templates\">\n",
    "                <div class=\"left\">\n",
    "                    <a href=\"/Public/CompiledDocument?meetingTemplateId=21847&compileOutputType=1\" class=\"dropdown-document-1\"><i> Document 1 </i></a>\n",
    "                    <a href=\"/Portal/CompiledDocument?meetingTemplateId=21847&compileOutputType=2\" class=\"dropdown-document-1\"><i> Document 1 </i></a>\n",
    "                </div>\n",
    "        </td>\n",
    "    </tr>\n",
    "</div>\n",
    "\"\"\"\n",
    "\n",
    "def extract_tags(html_content, tags_with_classes: list[tuple[str, str]]):\n",
    "    \"\"\"\n",
    "    This takes in HTML content and a list of tuples (tags, class_name),\n",
    "    and returns a string containing the text content of all elements with those tags and classes,\n",
    "    along with their href attribute if the tag is an \"a\" tag.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    text_parts = []\n",
    "\n",
    "    for tag, class_name in tags_with_classes:\n",
    "        if class_name:  # If a class name is specified\n",
    "            elements = soup.find_all(tag, class_=class_name)\n",
    "        else:  # If no class name is specified\n",
    "            elements = soup.find_all(tag)\n",
    "            \n",
    "        for element in elements:\n",
    "            text_content = element.get_text(strip=True)\n",
    "            # If the tag is a link (a tag), append its href as well\n",
    "            if tag == \"a\":\n",
    "                href = element.get('href', '')\n",
    "                text_parts.append(f\"{text_content} ({href})\" if href else text_content)\n",
    "            else:\n",
    "                text_parts.append(text_content)\n",
    "\n",
    "    return ' '.join(text_parts)\n",
    "#html_content = '''<div><p class=\"important\">Important text</p><a href=\"https://example.com\" class=\"link\">Example link</a></div>'''\n",
    "tags_with_classes = [('td'), ('a', 'link'), ('div', '')]\n",
    "\n",
    "extracted_text = extract_tags(html_content, tags_with_classes)\n",
    "print(extracted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from const import city_website_prefix_links, primegov_website_links\n",
    "from playwright.async_api import async_playwright\n",
    "\n",
    "# Provided HTML content\n",
    "html_content = \"\"\"\n",
    "<tbody>\n",
    "    <tr class=\"odd\">\n",
    "        <td class=\"meeting-title bodyTextColour\">Meeting Title 1</td>\n",
    "        <td class=\"bodyTextColour sorting_1\">Meeting Date 1</td>\n",
    "        <td class=\" bodyTextColour\">\n",
    "          <div class=\"docContainer archived-templates\">\n",
    "                <div class=\"left\">\n",
    "                    <a href=\"https://bonkers.com\" class=\"dropdown-document-1\">Document 1</a>\n",
    "                    <a href=\"https://billers.com\" class=\"dropdown-document-2\">Document 2</a>\n",
    "                </div>\n",
    "        </td>\n",
    "        <td class=\"meeting-title bodyTextColour\">Meeting Title</td>\n",
    "        <td class=\"bodyTextColour sorting_1\">Meeting Date</td>\n",
    "        <td class=\" bodyTextColour\">\n",
    "            <div class=\"docContainer archived-templates\">\n",
    "                <div class=\"left\">\n",
    "                    <a href=\"/Public/CompiledDocument?meetingTemplateId=21847&compileOutputType=1\" class=\"dropdown-document-1\"><i> Document 1 </i></a>\n",
    "                    <a href=\"/Public/CompiledDocument?meetingTemplateId=21847&compileOutputType=1\" class=\"dropdown-document-1\"><i> Document 1 </i></a>\n",
    "                </div>\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr class=\"odd\">\n",
    "        <td class=\"meeting-title bodyTextColour\">Meeting Title 2</td>\n",
    "        <td class=\"bodyTextColour sorting_1\">Meeting Date 2</td>\n",
    "        <td class=\" bodyTextColour\">\n",
    "          <div class=\"docContainer archived-templates\">\n",
    "                <div class=\"left\">\n",
    "                    <a href=\"https://juggs.com\" class=\"dropdown-document-1\">Document 1</a>\n",
    "                    <a href=\"https://jizz.com\" class=\"dropdown-document-2\">Document 2</a>\n",
    "                </div>\n",
    "        </td>\n",
    "        <td class=\"meeting-title bodyTextColour\">Meeting Title</td>\n",
    "        <td class=\"bodyTextColour sorting_1\">Meeting Date</td>\n",
    "        <td class=\" bodyTextColour\">\n",
    "            <div class=\"docContainer archived-templates\">\n",
    "                <div class=\"left\">\n",
    "                    <a href=\"/Public/CompiledDocument?meetingTemplateId=21847&compileOutputType=1\" class=\"dropdown-document-1\"><i> Document 1 </i></a>\n",
    "                    <a href=\"/Portal/CompiledDocument?meetingTemplateId=21847&compileOutputType=2\" class=\"dropdown-document-1\"><i> Document 1 </i></a>\n",
    "                </div>\n",
    "        </td>\n",
    "    </tr>\n",
    "</tbody>\n",
    "\"\"\"\n",
    "async def ascrape_primegov_playwright(url) -> str:\n",
    "    \"\"\"\n",
    "    An asynchronous Python function that uses Playwright to scrape\n",
    "    content from a given URL, extracting specified HTML tags based on tag names and optional class names.\n",
    "    \"\"\"\n",
    "    print(\"Started scraping...\")\n",
    "    results = \"\"\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=True)\n",
    "        try:\n",
    "            page = await browser.new_page()\n",
    "            await page.goto(url)\n",
    "\n",
    "            page_source = await page.content()\n",
    "\n",
    "            # Directly use extract_tags as there's no remove_unwanted_tags or remove_unessesary_lines\n",
    "            # function provided in the given context. Adjust if those functions exist and need to be integrated.\n",
    "            results = page_source\n",
    "            print(\"Content scraped\")\n",
    "        except Exception as e:\n",
    "            results = f\"Error: {e}\"\n",
    "        finally:\n",
    "            await browser.close()\n",
    "    return results\n",
    "\n",
    "def parse_and_extract(html_content, city_name=None):\n",
    "    # Corrected function for filtering href links\n",
    "    def filter_href_links(soup_element):\n",
    "        links = soup_element.find_all('a', href=True)\n",
    "        filtered_links = [link['href'] for link in links if any(substring in link['href'] for substring in [\"Public\", \"Portal\"])]\n",
    "        return filtered_links\n",
    "\n",
    "    # Parse HTML content\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # Initialize lists to hold extracted data\n",
    "    titles = []\n",
    "    dates = []\n",
    "    all_filtered_links = []\n",
    "\n",
    "    # Extract information\n",
    "    for tr in soup.find_all('tr', class_='odd' or 'even'):\n",
    "        title = tr.find('td', class_='meeting-title')\n",
    "        date = tr.find('td', class_='sorting_1')\n",
    "        \n",
    "        # Apply filter_href_links function to each tr to get filtered links\n",
    "        filtered_links = filter_href_links(tr)\n",
    "        \n",
    "        titles.append(title.text.strip() if title else 'N/A')\n",
    "        dates.append(date.text.strip() if date else 'N/A')\n",
    "        all_filtered_links.append(\", \".join(filtered_links) if filtered_links else 'N/A')\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'City Name': city_name if city_name else 'N/A', # Add city name if provided, else 'N/A\n",
    "        'Meeting Title': titles,\n",
    "        'Meeting Date': dates,\n",
    "        'Filtered Document Links': all_filtered_links\n",
    "    })\n",
    "\n",
    "    # now retun \n",
    "    return df\n",
    "\n",
    "def append_prefix_to_agenda_link(extracted_data, city_name):\n",
    "\n",
    "    prefix_link = city_website_prefix_links.get(city_name, \"\")\n",
    "\n",
    "    for item in extracted_data.iterrows():\n",
    "        item_data = item[1]  # Get the Series object from the row\n",
    "        # Check if 'Filtered Document Links' exists and is not 'N/A' before concatenating\n",
    "        if item_data['Filtered Document Links'] != 'N/A':\n",
    "            updated_links = [prefix_link + link for link in item_data['Filtered Document Links'].split(\", \")]\n",
    "            extracted_data.at[item[0], 'Filtered Document Links'] = \", \".join(updated_links)\n",
    "        else:\n",
    "            # Handle the case where 'Filtered Document Links' is missing or 'N/A'.\n",
    "            # Here, we're just leaving it as is, but you can adjust as needed.\n",
    "            pass\n",
    "        \n",
    "    return extracted_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = parse_and_extract(html_content, city_name=\"Santa Ana, CA\")\n",
    "df_with_prefix = append_prefix_to_agenda_link(df, \"Santa Ana, CA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City Name</th>\n",
       "      <th>Meeting Title</th>\n",
       "      <th>Meeting Date</th>\n",
       "      <th>Filtered Document Links</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Santa Ana, CA</td>\n",
       "      <td>Meeting Title 1</td>\n",
       "      <td>Meeting Date 1</td>\n",
       "      <td>https://santa-ana.primegov.com//Public/CompiledDocument?meetingTemplateId=21847&amp;compileOutputType=1, https://santa-ana.primegov.com//Public/CompiledDocument?meetingTemplateId=21847&amp;compileOutputType=1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Santa Ana, CA</td>\n",
       "      <td>Meeting Title 2</td>\n",
       "      <td>Meeting Date 2</td>\n",
       "      <td>https://santa-ana.primegov.com//Public/CompiledDocument?meetingTemplateId=21847&amp;compileOutputType=1, https://santa-ana.primegov.com//Portal/CompiledDocument?meetingTemplateId=21847&amp;compileOutputType=2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       City Name    Meeting Title    Meeting Date  \\\n",
       "0  Santa Ana, CA  Meeting Title 1  Meeting Date 1   \n",
       "1  Santa Ana, CA  Meeting Title 2  Meeting Date 2   \n",
       "\n",
       "                                                                                                                                                                                    Filtered Document Links  \n",
       "0  https://santa-ana.primegov.com//Public/CompiledDocument?meetingTemplateId=21847&compileOutputType=1, https://santa-ana.primegov.com//Public/CompiledDocument?meetingTemplateId=21847&compileOutputType=1  \n",
       "1  https://santa-ana.primegov.com//Public/CompiledDocument?meetingTemplateId=21847&compileOutputType=1, https://santa-ana.primegov.com//Portal/CompiledDocument?meetingTemplateId=21847&compileOutputType=2  "
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from playwright.async_api import async_playwright\n",
    "import asyncio\n",
    "from playwright.async_api import async_playwright\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "async def ascrape_primegov_playwright(url) -> str:\n",
    "    \"\"\"\n",
    "    Asynchronously scrape the content from a given URL using Playwright.\n",
    "\n",
    "    Args:\n",
    "    url (str): The URL to scrape.\n",
    "\n",
    "    Returns:\n",
    "    str: The HTML content of the page.\n",
    "    \"\"\"\n",
    "    print(\"Started scraping...\")\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=True)\n",
    "        try:\n",
    "            page = await browser.new_page()\n",
    "            await page.goto(url)\n",
    "            page_source = await page.content()\n",
    "            print(\"Content scraped\")\n",
    "            return page_source\n",
    "        except Exception as e:\n",
    "            print(f\"Scraping error: {e}\")\n",
    "            return \"\"\n",
    "        finally:\n",
    "            await browser.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_and_extract(html_content, city_name=None):\n",
    "    \"\"\"\n",
    "    Parse the HTML content and extract meeting titles, dates, and filtered document links.\n",
    "\n",
    "    Args:\n",
    "    html_content (str): HTML content to parse.\n",
    "    city_name (str, optional): Name of the city for which data is extracted. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: A pandas DataFrame containing the extracted data.\n",
    "    \"\"\"\n",
    "    def filter_href_links(soup_element):\n",
    "        links = soup_element.find_all('a', href=True)\n",
    "        filtered_links = [link['href'] for link in links if any(substring in link['href'] for substring in [\"Public\", \"Portal\"])]\n",
    "        return filtered_links\n",
    "\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    titles = []\n",
    "    dates = []\n",
    "    all_filtered_links = []\n",
    "\n",
    "    for tr in soup.find_all('tr'):\n",
    "        title = tr.find('td', class_=' meeting-title bodyTextColour')\n",
    "        date = tr.find('td', class_='bodyTextColour sorting_1')\n",
    "        filtered_links = filter_href_links(tr)\n",
    "        \n",
    "        titles.append(title.text.strip() if title else 'N/A')\n",
    "        dates.append(date.text.strip() if date else 'N/A')\n",
    "        all_filtered_links.append(\", \".join(filtered_links) if filtered_links else 'N/A')\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'City Name': city_name if city_name else 'N/A',\n",
    "        'Meeting Title': titles,\n",
    "        'Meeting Date': dates,\n",
    "        'Filtered Document Links': all_filtered_links\n",
    "    })\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_prefix_to_agenda_link(extracted_data, city_name):\n",
    "    \"\"\"\n",
    "    Append a prefix to each 'Filtered Document Link' in the extracted data based on the city name.\n",
    "\n",
    "    Args:\n",
    "    extracted_data (DataFrame): The extracted data DataFrame.\n",
    "    city_name (str): The name of the city to use for prefixing links.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: The DataFrame with updated 'Filtered Document Links'.\n",
    "    \"\"\"\n",
    "    prefix_link = city_website_prefix_links.get(city_name, \"\")\n",
    "    for index, row in extracted_data.iterrows():\n",
    "        if row['Filtered Document Links'] != 'N/A':\n",
    "            updated_links = [prefix_link + link for link in row['Filtered Document Links'].split(\", \")]\n",
    "            extracted_data.at[index, 'Filtered Document Links'] = \", \".join(updated_links)\n",
    "    return extracted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "asyncio.run() cannot be called from a running event loop",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[108], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Run the async main function\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 12\u001b[0m     \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files\\Python311\\Lib\\asyncio\\runners.py:186\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(main, debug)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the coroutine and return the result.\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \n\u001b[0;32m    163\u001b[0m \u001b[38;5;124;03mThis function runs the passed coroutine, taking care of\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;124;03m    asyncio.run(main())\u001b[39;00m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m events\u001b[38;5;241m.\u001b[39m_get_running_loop() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;66;03m# fail fast with short traceback\u001b[39;00m\n\u001b[1;32m--> 186\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    187\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124masyncio.run() cannot be called from a running event loop\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Runner(debug\u001b[38;5;241m=\u001b[39mdebug) \u001b[38;5;28;01mas\u001b[39;00m runner:\n\u001b[0;32m    190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m runner\u001b[38;5;241m.\u001b[39mrun(main)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: asyncio.run() cannot be called from a running event loop"
     ]
    }
   ],
   "source": [
    "async def main():\n",
    "    city_urls=primegov_website_links\n",
    "    for city_name, url in city_urls.items():\n",
    "        html_content = await ascrape_primegov_playwright(url)\n",
    "        if html_content:\n",
    "            df = parse_and_extract(html_content, city_name)\n",
    "            df_with_prefix = append_prefix_to_agenda_link(df, city_name)\n",
    "            print(df_with_prefix)\n",
    "\n",
    "# Run the async main function\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import pprint\n",
    "from bs4 import BeautifulSoup\n",
    "from playwright.async_api import async_playwright\n",
    "from const import city_website_prefix_links, tags\n",
    "\n",
    "async def ascrape_primegov_playwright(url, city_name=False):\n",
    "    print(f\"Started scraping {city_name}...\")\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=True)\n",
    "        try:\n",
    "            page = await browser.new_page()\n",
    "            await page.goto(url)\n",
    "            page_source = await page.content()\n",
    "            results = parse_and_extract(page_source)  # Ensure this returns a DataFrame\n",
    "            print(\"Content scraped\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            return None\n",
    "        finally:\n",
    "            await browser.close()\n",
    "\n",
    "    if results is not None:\n",
    "        # Adjust file name and ensure directory exists\n",
    "        file_name = f'scraped_{city_name}_content.xlsx'\n",
    "        results.to_excel(file_name, index=False)  # Save DataFrame to Excel\n",
    "        print(f\"Results saved to {file_name}\")\n",
    "    else:\n",
    "        print(\"No results to save.\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "from pydantic import BaseModel\n",
    "from typing import Optional\n",
    "from scrape import ascrape_playwright, append_prefix_to_agenda_link, filter_commissions, ascrape_primegov_playwright\n",
    "from ai_extractor import extract\n",
    "from const import city_website_prefix_links, primegov_website_links, tags\n",
    "from schemas import SchemaCityWebsites, PrimegovCityInfo\n",
    "import pprint\n",
    "\n",
    "  \n",
    "async def scrape_primegov_with_playwright(url: str, city_name: str):\n",
    "    html_content = await ascrape_primegov_playwright(url, city_name)\n",
    "\n",
    "async def main():\n",
    "    city_commissions_results = {}\n",
    "    # Assuming primegov_website_links is a dictionary {city_name: city_url, ...}\n",
    "    for city_name, city_url in primegov_website_links.items():\n",
    "        commissions = await scrape_primegov_with_playwright(city_url, city_name)  # removed schema_pydantic for simplicity\n",
    "        city_commissions_results[city_name] = commissions\n",
    "    print(city_commissions_results)\n",
    "\n",
    "# Use get_event_loop to handle existing loops, especially in Jupyter notebooks\n",
    "if __name__ == \"__main__\":\n",
    "    loop = asyncio.get_event_loop()\n",
    "    if loop.is_running():\n",
    "        # If the loop is running, tasks should be scheduled with create_task or similar\n",
    "        task = loop.create_task(main())\n",
    "        # Optionally wait for the task if necessary\n",
    "    else:\n",
    "        asyncio.run(main())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", , , Meeting Title, , , Meeting Title, Meeting Date, Document 1 (https://example.com), Document 2 (https://example.com), Meeting Title, Meeting Date, , Meeting Title, Meeting Date, , Meeting Title, Meeting Date, Document 1 (https://example.com), Document 2 (https://example.com)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recycle Bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import pprint\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from playwright.async_api import async_playwright\n",
    "from const import city_website_prefix_links, tags\n",
    "\n",
    "async def ascrape_primegov_playwright(url, tags: list[dict] = tags) -> str:\n",
    "    \"\"\"\n",
    "    An asynchronous Python function that uses Playwright to scrape\n",
    "    content from a given URL, extracting specified HTML tags based on tag names and optional class names.\n",
    "    \"\"\"\n",
    "    print(\"Started scraping...\")\n",
    "    results = \"\"\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=True)\n",
    "        try:\n",
    "            page = await browser.new_page()\n",
    "            await page.goto(url)\n",
    "\n",
    "            page_source = await page.content()\n",
    "\n",
    "            # Directly use extract_tags as there's no remove_unwanted_tags or remove_unessesary_lines\n",
    "            # function provided in the given context. Adjust if those functions exist and need to be integrated.\n",
    "            results = extract_nested_structure(page_source, tags=tags)\n",
    "            print(\"Content scraped\")\n",
    "        except Exception as e:\n",
    "            results = f\"Error: {e}\"\n",
    "        finally:\n",
    "            await browser.close()\n",
    "    \n",
    "    print(\"Scraping Complete, saving results to text file...\")\n",
    "\n",
    "    with open('scraped_content.txt', 'w', encoding='utf-8') as file:\n",
    "        file.write(results)\n",
    "\n",
    "    print(\"Results saved to scraped_content.txt\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "from pydantic import BaseModel\n",
    "from typing import Optional\n",
    "from scrape import ascrape_playwright, append_prefix_to_agenda_link, filter_commissions, ascrape_primegov_playwright\n",
    "from ai_extractor import extract\n",
    "from const import city_website_prefix_links, legistar_website_links, primegov_website_links, tags\n",
    "from schemas import SchemaCityWebsites\n",
    "import pprint\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    async def scrape_with_playwright(url: str, city_name: str, tags=None, **kwargs):\n",
    "        html_content = await ascrape_primegov_playwright(url, tags)\n",
    "\n",
    "        print(f\"Extracting {city_name} content with LLM\")\n",
    "\n",
    "        all_extracted_content = []\n",
    "        token_limit = 4000  # Set a token limit for the content\n",
    "\n",
    "        num_chunks = len(html_content) // token_limit + (1 if len(html_content) % token_limit > 0 else 0)\n",
    "        for i in range(num_chunks):\n",
    "            start_index = i * token_limit\n",
    "            end_index = start_index + token_limit\n",
    "            html_content_chunk = html_content[start_index:end_index]\n",
    "\n",
    "            extracted_content = extract(**kwargs, content=html_content_chunk)\n",
    "            processed_content = filter_commissions(append_prefix_to_agenda_link(extracted_content, city_name))\n",
    "            all_extracted_content.extend(processed_content)\n",
    "\n",
    "            await asyncio.sleep(2) \n",
    "\n",
    "        return all_extracted_content\n",
    "\n",
    "\n",
    "    city_commissions_results = {}\n",
    "    loop = asyncio.get_event_loop()\n",
    "\n",
    "    # Legistar website links legistar_website_prefix_links, legistar_, primegov,legistar,primegov,[\"td\", \"span\", \"a\"]\n",
    "\n",
    "    for city_name, city_url in primegov_website_links.items():\n",
    "        commissions = loop.run_until_complete(scrape_with_playwright(city_url, city_name, tags=tags, schema_pydantic=SchemaCityWebsites))\n",
    "        city_commissions_results[city_name] = commissions\n",
    "\n",
    "        # Save the results to a JSON file\n",
    "    with open('city_commissions_2.json', 'w', encoding='utf-8') as file:\n",
    "        json.dump(city_commissions_results, file, ensure_ascii=False, indent=4, default=lambda x: x.dict() if isinstance(x, BaseModel) else x)\n",
    "\n",
    "    print(\"Data has been saved to city_commissions.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_nested_structure(html_content):\n",
    "    \"\"\"\n",
    "    This takes in HTML content and returns a list of strings, where each string represents\n",
    "    the text content and hrefs for a sequence of tags: td for title, td for date, \n",
    "    and then nested a tags inside another td for documents.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    structured_data = []\n",
    "\n",
    "    # Find all 'td' elements with class 'bodyTextColour' that are parents of 'a' tags\n",
    "    document_containers = soup.find_all('td', class_='bodyTextColour')\n",
    "    \n",
    "    for container in document_containers:\n",
    "        # Extract the title and date from previous siblings\n",
    "        title_td = container.find_previous_sibling('td', class_='meeting-title bodyTextColour')\n",
    "        date_td = container.find_previous_sibling('td', class_='bodyTextColour sorting_1')\n",
    "        title = title_td.get_text(strip=True) if title_td else ''\n",
    "        date = date_td.get_text(strip=True) if date_td else ''\n",
    "\n",
    "        # Find the 'a' tags within the container\n",
    "        document_links = container.find_all('a', class_=['dropdown-document-1', 'dropdown-document-2'])\n",
    "\n",
    "        # Extract text and href for each document link\n",
    "        documents = [f\"{doc.get_text(strip=True)} ({doc['href']})\" for doc in document_links]\n",
    "\n",
    "        # Combine the data\n",
    "        structured_data.append(f\"{title}, {date}, \" + \", \".join(documents))\n",
    "        \n",
    "        # turn list into string\n",
    "        structured_data_string = ', '.join(structured_data)\n",
    "\n",
    "    return structured_data_string\n",
    "\n",
    "results = extract_nested_structure(html_content)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_primegov_tags_1(html_content, tags: list[dict]):\n",
    "    \"\"\"\n",
    "    This takes in HTML content and a list of tag specifications, where each specification is a dictionary\n",
    "    that contains the tag name and an optional class name. It returns a string containing the text content\n",
    "    of all elements with those tags, including their href attribute if the tag is an \"a\" tag.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    text_parts = []\n",
    "\n",
    "    for tag_spec in tags:\n",
    "        tag_name = tag_spec.get(\"name\")\n",
    "        class_name = tag_spec.get(\"class\")\n",
    "        if class_name:\n",
    "            elements = soup.find_all(tag_name, class_=class_name)\n",
    "        else:\n",
    "            elements = soup.find_all(tag_name)\n",
    "\n",
    "        for element in elements:\n",
    "            text_content = element.get_text(strip=True)\n",
    "            if tag_name == \"a\":\n",
    "                href = element.get('href', '')\n",
    "                text_parts.append(f\"{text_content} ({href})\" if href else text_content)\n",
    "            else:\n",
    "                text_parts.append(text_content)\n",
    "\n",
    "    return ' '.join(text_parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def ascrape_primegov_playwright(url, tags: list[dict] = tags) -> str:\n",
    "    \"\"\"\n",
    "    An asynchronous Python function that uses Playwright to scrape\n",
    "    content from a given URL, extracting specified HTML tags based on tag names and optional class names.\n",
    "    \"\"\"\n",
    "    print(\"Started scraping...\")\n",
    "    results = \"\"\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=True)\n",
    "        try:\n",
    "            page = await browser.new_page()\n",
    "            await page.goto(url)\n",
    "\n",
    "            page_source = await page.content()\n",
    "\n",
    "            # Directly use extract_tags as there's no remove_unwanted_tags or remove_unessesary_lines\n",
    "            # function provided in the given context. Adjust if those functions exist and need to be integrated.\n",
    "            results = extract_primegov_tags_1(page_source, tags=tags)\n",
    "            print(\"Content scraped\")\n",
    "        except Exception as e:\n",
    "            results = f\"Error: {e}\"\n",
    "        finally:\n",
    "            await browser.close()\n",
    "    \n",
    "    print(\"Scraping Complete, saving results to text file...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_legistar_tags(html_content, tags: list[str]):\n",
    "    \"\"\"\n",
    "    This takes in HTML content and a list of tags, and returns a string\n",
    "    containing the text content of all elements with those tags, along with their href attribute if the\n",
    "    tag is an \"a\" tag.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    all_titles = []\n",
    "    all_hrefs = []\n",
    "    for tag in tags:\n",
    "        elements = soup.find_all(tag)\n",
    "\n",
    "        for element in elements:\n",
    "            print(element.get_text())\n",
    "            \"\"\" # if the tag is a td and has a title attribute, append its title, if it has no title attribute but has a date . Append the date\n",
    "            if tag == \"td\":\n",
    "                title = element.get('title')\n",
    "                if title:\n",
    "                    all_titles.append(element.get_text())\"\"\"\n",
    "\n",
    "            # check if the tag is 'a' \n",
    "            if tag == \"a\":\n",
    "                # Get class attribute and check if it starts with \"dropdown-document-\" \n",
    "                class_attr = element.get('class')\n",
    "\n",
    "                if class_attr:\n",
    "                    # First, try to find elements with \"AgendaPacket\".\n",
    "                    if any(\"AgendaPacket\" in c for c in class_attr):\n",
    "                        href = element.get('href')\n",
    "                        all_hrefs.append(href)\n",
    "                        # If no \"AgendaPacket\" found, then look for \"Agenda\".\n",
    "                    elif any(\"Agenda\" in c for c in class_attr):  # Adjusted to look for \"Agenda\"\n",
    "                        href = element.get('href')\n",
    "                        all_hrefs.append(href)\n",
    "                    \n",
    "                    elif any(\"hypBody\" in c for c in class_attr):\n",
    "                        title = element.get_text()\n",
    "                        all_titles.append(title)\n",
    "\n",
    "    return all_titles, all_hrefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Now, look for corresponding 'AgendaPacket' or 'Agenda' for each title\n",
    "    for i, title in enumerate(all_titles):\n",
    "        found = False\n",
    "        for a_tag in soup.find_all('a', id=lambda x: x and (\"AgendaPacket\" in x or \"Agenda\" in x)):\n",
    "            if \"AgendaPacket\" in a_tag.get('id', ''):\n",
    "                all_hrefs[i] = a_tag.get('href')\n",
    "                found = True\n",
    "                print(f\"Matched 'AgendaPacket' href to title '{title}': {all_hrefs[i]}\")\n",
    "                break  # Prefer 'AgendaPacket' over 'Agenda'\n",
    "        \n",
    "        if not found:  # If 'AgendaPacket' not found, look for 'Agenda'\n",
    "            for a_tag in soup.find_all('a', id=lambda x: x and \"Agenda\" in x):\n",
    "                if \"Agenda\" in a_tag.get('id', '') and not \"AgendaPacket\" in a_tag.get('id', ''):\n",
    "                    all_hrefs[i] = a_tag.get('href')\n",
    "                    found = True\n",
    "                    print(f\"Matched 'Agenda' href to title '{title}': {all_hrefs[i]}\")\n",
    "                    break\n",
    "        \n",
    "        if not found:  # If neither 'AgendaPacket' nor 'Agenda' found\n",
    "            all_hrefs[i] = \"no Agenda\"\n",
    "            print(f\"No 'AgendaPacket' or 'Agenda' found for title '{title}'. Setting href as 'no Agenda'.\")\n",
    "\n",
    "    print(\"Completed processing. Function execution finished.\")\n",
    "\n",
    "    return all_titles, all_hrefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_legistar_tags(html_content):\n",
    "    \"\"\"\n",
    "    Parses HTML content to find 'a' tags with specific keywords (\"hypBody\", \"Agenda\", \"AgendaPacket\") in their IDs,\n",
    "    adding time stops and print statements to ensure content loads correctly.\n",
    "    For each 'hypBody' found, it tries to find a corresponding 'AgendaPacket'. If not found, it looks for 'Agenda'.\n",
    "    If neither is found, it adds \"no Agenda\" as a placeholder.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    print(\"Soup object created. Starting to parse 'a' tags...\")\n",
    "\n",
    "    all_titles = []  # List to store titles from \"hypBody\"\n",
    "    all_hrefs = []   # List to store hrefs based on the conditions\n",
    "\n",
    "    # Search and process 'a' tags for \"hypBody\"\n",
    "    for a_tag in soup.find_all('a', id=lambda x: x and \"hypBody\" in x):\n",
    "        tag_id = a_tag.get('id', '')\n",
    "        title = a_tag.get_text(strip=True)\n",
    "        print(f\"Processing 'a' tag with ID: {tag_id}\")\n",
    "        time.sleep(.25)  # Add a time stop to observe the process\n",
    "\n",
    "        if title:\n",
    "            all_titles.append(title)\n",
    "            # Placeholder for href, to be updated later\n",
    "            all_hrefs.append(None)\n",
    "            print(f\"Added title from 'hypBody': {title}\")\n",
    "    \n",
    "    # Search and process 'a' tags for \"AgendaPacket\"\n",
    "    for a_tag in soup.find_all('a', id=lambda x: x and \"AgendaPacket\" in x):\n",
    "        tag_id = a_tag.get('id', '')\n",
    "        href = a_tag.get('href', '')\n",
    "        print(f\"Processing 'a' tag with ID: {tag_id}\")\n",
    "        time.sleep(.25)  # Add a time stop to observe the process\n",
    "    \n",
    "        if href:\n",
    "            index = all_titles.index(a_tag.get_text(strip=True))\n",
    "            all_hrefs[index] = href\n",
    "            print(f\"Added href from 'AgendaPacket': {href}\")\n",
    "    \n",
    "    # Search and process 'a' tags for \"Agenda\"\n",
    "    for a_tag in soup.find_all('a', id=lambda x: x and \"Agenda\" in x):\n",
    "        tag_id = a_tag.get('id', '')\n",
    "        href = a_tag.get('href', '')\n",
    "        print(f\"Processing 'a' tag with ID: {tag_id}\")\n",
    "        time.sleep(.25)  # Add a time stop to observe the process\n",
    "\n",
    "        if href:\n",
    "            index = all_titles.index(a_tag.get_text(strip=True))\n",
    "            all_hrefs[index] = href\n",
    "            print(f\"Added href from 'Agenda': {href}\")\n",
    "\n",
    "    return all_titles, all_hrefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_legistar_tags(html_content):\n",
    "    \"\"\"\n",
    "    Parses HTML content to find 'a' tags with specific keywords (\"hypBody\", \"Agenda\", \"AgendaPacket\") in their IDs,\n",
    "    adding time stops and print statements to ensure content loads correctly.\n",
    "    Directly associates 'hypBody' tags with either 'AgendaPacket' or 'Agenda' based on availability, without using find_next_siblings.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    print(\"Soup object created. Starting to parse 'a' tags...\")\n",
    "\n",
    "    hypBody_map = {}  # Map to store titles from \"hypBody\" to hrefs\n",
    "\n",
    "    # First pass: identify all \"hypBody\" tags and initialize map entries with None\n",
    "    for a_tag in soup.find_all('a', id=lambda x: x and \"hypBody\" in x):\n",
    "        title = a_tag.get_text(strip=True)\n",
    "        if title not in hypBody_map:\n",
    "            hypBody_map[title] = None  # Initialize with None to indicate no href found yet\n",
    "            print(f\"Identified 'hypBody' with title: {title}\")\n",
    "\n",
    "    # Second pass: update href for \"AgendaPacket\" or \"Agenda\" if they match titles in hypBody_map\n",
    "    for a_tag in soup.find_all('a', id=lambda x: x and (\"AgendaPacket\" in x or \"Agenda\" in x)):\n",
    "        title = a_tag.get_text(strip=True)\n",
    "        href = a_tag.get('href', '')\n",
    "        if title in hypBody_map:\n",
    "            # Prioritize \"AgendaPacket\" over \"Agenda\" by not overwriting if already found\n",
    "            if \"AgendaPacket\" in a_tag.get('id', '') or (hypBody_map[title] is None and \"Agenda\" in a_tag.get('id', '')):\n",
    "                hypBody_map[title] = href\n",
    "                print(f\"Updated href for '{title}' with {a_tag.get('id', '')}: {href}\")\n",
    "\n",
    "    # Prepare the final lists to return\n",
    "    all_titles = list(hypBody_map.keys())\n",
    "    all_hrefs = [href if href is not None else \"no Agenda\" for href in hypBody_map.values()]\n",
    "\n",
    "    return all_titles, all_hrefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_unwanted_tags(html_content, unwanted_tags=[\"script\", \"style\"]):\n",
    "    \"\"\"\n",
    "    This removes unwanted HTML tags from the given HTML content.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    for tag in unwanted_tags:\n",
    "        for element in soup.find_all(tag):\n",
    "            element.decompose()\n",
    "\n",
    "    print(\"Unwanted tags removed\")\n",
    "\n",
    "    return str(soup)\n",
    "\n",
    "\n",
    "def extract_tags(html_content, tags: list[str]):\n",
    "    \"\"\"\n",
    "    This takes in HTML content and a list of tags, and returns a string\n",
    "    containing the text content of all elements with those tags, along with their href attribute if the\n",
    "    tag is an \"a\" tag.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    text_parts = []\n",
    "\n",
    "    for tag in tags:\n",
    "        elements = soup.find_all(tag)\n",
    "        for element in elements:\n",
    "            # If the tag is a link (a tag), append its href as well\n",
    "            if tag == \"a\":\n",
    "                href = element.get('href')\n",
    "                if href:\n",
    "                    text_parts.append(f\"{element.get_text()} ({href})\")\n",
    "                else:\n",
    "                    text_parts.append(element.get_text())\n",
    "            else:\n",
    "                text_parts.append(element.get_text())\n",
    "\n",
    "    return ' '.join(text_parts)\n",
    "\n",
    "\n",
    "def remove_unnecessary_lines(content):\n",
    "    # Split content into lines\n",
    "    lines = content.split(\"\\n\")\n",
    "\n",
    "    # Strip whitespace for each line\n",
    "    stripped_lines = [line.strip() for line in lines]\n",
    "\n",
    "    # Filter out empty lines\n",
    "    non_empty_lines = [line for line in stripped_lines if line]\n",
    "\n",
    "    # Remove duplicated lines (while preserving order)\n",
    "    seen = set()\n",
    "    deduped_lines = [line for line in non_empty_lines if not (\n",
    "        line in seen or seen.add(line))]\n",
    "\n",
    "    # Join the cleaned lines without any separators (remove newlines)\n",
    "    cleaned_content = \"\".join(deduped_lines)\n",
    "\n",
    "    return cleaned_content\n",
    "\n",
    "\n",
    "def append_prefix_to_agenda_link(extracted_data, city_name):\n",
    "    prefix_link = city_website_prefix_links.get(city_name, \"\")\n",
    "    for item in extracted_data:\n",
    "        # Check if 'agenda_link' exists and is not None before concatenating\n",
    "        if item.get('agenda_link') is not None:\n",
    "            item['agenda_link'] = prefix_link + item['agenda_link']\n",
    "        else:\n",
    "            # Handle the case where 'agenda_link' is missing or None. \n",
    "            # Decide on appropriate action, such as setting a default value, skipping, or logging.\n",
    "            # For example, setting a default value or placeholder (adjust as needed):\n",
    "            item['agenda_link'] = \"No Agenda Link Available\"  # or \"No agenda link available\"\n",
    "    return extracted_data\n",
    "\n",
    "\n",
    "def filter_commissions(commissions):\n",
    "    \"\"\"\n",
    "    This function filters a list of commissions to only include those with names, if none are present replace with a default value.\n",
    "    \"\"\"\n",
    "    filtered_commissions = [\n",
    "    item for item in commissions \n",
    "    if item['commission_name'] and (\"plann\" in item['commission_name'].lower() or \"hous\" in item['commission_name'].lower())]\n",
    "    \n",
    "    print(\"Commissions filtered\")\n",
    "\n",
    "    return filtered_commissions\n",
    "\n",
    "\n",
    "async def ascrape_playwright(url, tags: list[str] = [\"h1\", \"h2\", \"h3\", \"td\", \"span\", \"a\"]) -> str:\n",
    "    \"\"\"\n",
    "    An asynchronous Python function that uses Playwright to scrape\n",
    "    content from a given URL, extracting specified HTML tags and removing unwanted tags and unnecessary\n",
    "    lines.\n",
    "    \"\"\"\n",
    "    print(\"Started scraping...\")\n",
    "    results = \"\"\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=True)\n",
    "        try:\n",
    "            page = await browser.new_page()\n",
    "            await page.goto(url)\n",
    "\n",
    "            page_source = await page.content()\n",
    "\n",
    "            results = remove_unnecessary_lines(extract_tags(remove_unwanted_tags(\n",
    "                page_source), tags))\n",
    "            print(\"Content scraped\")\n",
    "        except Exception as e:\n",
    "            results = f\"Error: {e}\"\n",
    "        await browser.close()\n",
    "    \n",
    "    print(\"Scraping Complete, saving results to text file...\")\n",
    "\n",
    "    with open('scraped_content.txt', 'w', encoding='utf-8') as file:\n",
    "        file.write(results)\n",
    "\n",
    "    print(\"Results saved to scraped_content.txt\")\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_links(html_content):\n",
    "    \"\"\"\n",
    "    Extracts all links from the given HTML content.\n",
    "\n",
    "    Args:\n",
    "        html_content (str): HTML content of a page.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of href values from <a> tags.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    print(\"Extracted links from HTML content\")\n",
    "    return [a['href'] for a in soup.find_all('a') if 'href' in a.attrs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_agenda_links_with_pdfs_and_details(data):\n",
    "    \"\"\"\n",
    "    Updates the input JSON data structure with PDF links and/or project details\n",
    "    extracted from each agenda link.\n",
    "\n",
    "    Args:\n",
    "        data (dict): The original JSON data structure.\n",
    "\n",
    "    Returns:\n",
    "        dict: The updated JSON data structure with PDF links or project details.\n",
    "    \"\"\"\n",
    "    for city, agendas in data.items():\n",
    "        for agenda in agendas:\n",
    "            agenda_link = agenda['agenda_link']\n",
    "            if is_pdf_link(agenda_link):\n",
    "                pdf_path = download_pdf(agenda_link)\n",
    "                agenda['pdf_link'] = str(pdf_path) if pdf_path else \"Failed to download\"\n",
    "            else:\n",
    "                html_content = fetch_html(agenda_link)\n",
    "                project_details = extract_links(html_content)\n",
    "                agenda['project_details'] = project_details\n",
    "\n",
    "    return data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
