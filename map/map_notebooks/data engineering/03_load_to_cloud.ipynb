{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We are going to use PostGreSQL for the ETL pipeline\n",
    "### 1. Create a database in PostGreSQL\n",
    "### 2. Use psycopg2 to connect to the database\n",
    "### 3. Use boto3 to connect to S3\n",
    "### 4. Create tables in the database\n",
    "### 5. Load data from S3 to the database\n",
    "### 6. Run quality checks on the data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#psycopg2\n",
    "#boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import psycopg2\n",
    "from psycopg2 import sql\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# --- Step 2: Store Data in S3 ---\n",
    "def upload_to_s3(file_path, bucket_name, object_name=None):\n",
    "    s3 = boto3.client('s3')\n",
    "    \n",
    "    if object_name is None:\n",
    "        object_name = file_path.split('/')[-1]\n",
    "        \n",
    "    try:\n",
    "        s3.upload_file(file_path, bucket_name, object_name)\n",
    "        logging.info(f\"Uploaded {file_path} to {bucket_name}/{object_name}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error uploading {file_path} to {bucket_name}/{object_name}. Error: {e}\")\n",
    "\n",
    "# --- Step 3: Prepare AWS PostgreSQL Database ---\n",
    "def connect_to_postgres():\n",
    "    return psycopg2.connect(\n",
    "        dbname=\"your_db_name\",\n",
    "        user=\"your_user\",\n",
    "        password=\"your_password\",\n",
    "        host=\"your_host\",\n",
    "        port=\"your_port\"\n",
    "    )\n",
    "\n",
    "def create_table_in_postgres():\n",
    "    with connect_to_postgres() as conn:\n",
    "        with conn.cursor() as cur:\n",
    "            # Adjust this SQL according to your schema\n",
    "            create_table_sql = \"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS your_table_name (\n",
    "                column1 data_type1,\n",
    "                column2 data_type2,\n",
    "                ...\n",
    "            );\n",
    "            \"\"\"\n",
    "            cur.execute(create_table_sql)\n",
    "            conn.commit()\n",
    "            logging.info(\"Table created successfully in PostgreSQL.\")\n",
    "\n",
    "# --- Step 4: Load Data from S3 to PostgreSQL ---\n",
    "def load_data_from_s3_to_postgres(bucket_name, object_name):\n",
    "    s3 = boto3.client('s3')\n",
    "    obj = s3.get_object(Bucket=bucket_name, Key=object_name)\n",
    "    data = pd.read_csv(obj['Body'])\n",
    "    \n",
    "    with connect_to_postgres() as conn:\n",
    "        data.to_sql('your_table_name', conn, index=False, if_exists='replace', method='multi')\n",
    "        logging.info(\"Data loaded successfully from S3 to PostgreSQL.\")\n",
    "\n",
    "# --- Step 5: Data Validation ---\n",
    "def run_validation_checks():\n",
    "    with connect_to_postgres() as conn:\n",
    "        with conn.cursor() as cur:\n",
    "            # Sample validation check: Adjust accordingly\n",
    "            cur.execute(\"SELECT COUNT(*) FROM your_table_name WHERE your_column IS NULL;\")\n",
    "            null_count = cur.fetchone()[0]\n",
    "            \n",
    "            if null_count > 0:\n",
    "                raise ValueError(f\"There are {null_count} null values in your_column.\")\n",
    "            logging.info(\"Data validation successful.\")\n",
    "\n",
    "# --- Step 6: Monitoring & Maintenance ---\n",
    "def backup_postgres():\n",
    "    # Implement backup logic here. This often depends on your infrastructure.\n",
    "    # You may use tools like `pg_dump` or AWS-native solutions for RDS backups.\n",
    "    pass\n",
    "\n",
    "def cleanup_old_s3_files(bucket_name, age_days):\n",
    "    s3 = boto3.resource('s3')\n",
    "    bucket = s3.Bucket(bucket_name)\n",
    "    \n",
    "    # Logic to delete files older than age_days\n",
    "    for obj in bucket.objects.all():\n",
    "        if (obj.last_modified - pd.Timestamp.now()).days > age_days:\n",
    "            obj.delete()\n",
    "            logging.info(f\"Deleted old file {obj.key} from S3.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Adjust these parameters accordingly\n",
    "    file_path = \"path_to_your_data_file.csv\"\n",
    "    bucket_name = \"your_s3_bucket_name\"\n",
    "    \n",
    "    # Upload to S3\n",
    "    upload_to_s3(file_path, bucket_name)\n",
    "    \n",
    "    # Setup PostgreSQL Table\n",
    "    create_table_in_postgres()\n",
    "    \n",
    "    # Load data from S3 to PostgreSQL\n",
    "    object_name = file_path.split('/')[-1]  # Assuming object_name is filename\n",
    "    load_data_from_s3_to_postgres(bucket_name, object_name)\n",
    "    \n",
    "    # Validate data\n",
    "    run_validation_checks()\n",
    "    \n",
    "    # Backup PostgreSQL\n",
    "    backup_postgres()\n",
    "    \n",
    "    # Cleanup old S3 files\n",
    "    cleanup_old_s3_files(bucket_name, age_days=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
